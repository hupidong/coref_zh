# Chinese model configuration.

bert_base_chinese {
  # Edit this
  data_dir = ./data
  
  # Computation limits.
  max_top_antecedents = 50
  max_training_sentences = 11
  top_span_ratio = 0.4
  max_num_speakers = 20
  max_segment_len = 128

  # Learning
  bert_learning_rate = 1e-05
  task_learning_rate = 0.0002
  adam_eps = 1e-6
  dropout_rate = 0.3
  
  # Task choice
  num_docs = 1810
  num_epochs = 30
  do_train = true
  do_eval = true
  do_test = true
  do_one_example_test = true
  eval_frequency = 5000
  report_frequency = 10

  # Model hyperparameters.
  genres = ["bc", "bn", "mz", "nw", "tc", "wb"]
  coref_depth = 2
  ffnn_size = 2000
  feature_size = 20
  max_span_width = 30
  use_metadata = true
  use_features = true
  use_segment_distance = true
  model_heads = false
  fine_grained = true
  use_prior = true
  single_example = true

  # file path
  train_path = ${data_dir}/train/train.chinese.128.jsonlines
  eval_path = ${data_dir}/dev/dev.chinese.128.jsonlines
  test_path = ${data_dir}/test/test.chinese.128.jsonlines
  test_output_path = ${data_dir}/test_result.jsonlines
  online_output_path = ${data_dir}/online_test_result.jsonlines
  conll_eval_path = ${data_dir}/dev/dev.chinese.v4_gold_conll
  conll_test_path= ${data_dir}/test/test.chinese.v4_gold_conll
  model_save_path = ./trained_coref_model
  pretrained_model = ./pretrain_model/bert-base-chinese
  vocab_file = ./pretrain_model/bert-base-chinese/vocab.txt
  bert_config_file = ./pretrain_model/bert-base-chinese/config.json
}
roberta_L6_H768 {
  # Edit this
  data_dir = ./data

  # Computation limits.
  max_top_antecedents = 50
  max_training_sentences = 11
  top_span_ratio = 0.4
  max_num_speakers = 20
  max_segment_len = 128

  # Learning
  bert_learning_rate = 1e-05
  task_learning_rate = 0.0002
  adam_eps = 1e-6
  dropout_rate = 0.3

  # Task choice
  num_docs = 1810
  num_epochs = 30
  do_train = true
  do_eval = true
  do_test = false
  do_one_example_test = true
  eval_frequency = 2000
  report_frequency = 10

  # Model hyperparameters.
  genres = ["bc", "bn", "mz", "nw", "tc", "wb"]
  coref_depth = 2
  ffnn_size = 2000
  feature_size = 20
  max_span_width = 30
  use_metadata = true
  use_features = true
  use_segment_distance = true
  model_heads = false
  fine_grained = true
  use_prior = true
  single_example = true

  # file path
  train_path = ${data_dir}/train/train.chinese.128.jsonlines
  eval_path = ${data_dir}/dev/dev.chinese.128.jsonlines
  test_path = ${data_dir}/test/test.chinese.128.jsonlines
  test_output_path = ${data_dir}/test_result.jsonlines
  online_output_path = ${data_dir}/online_test_result.jsonlines
  conll_eval_path = ${data_dir}/dev/dev.chinese.v4_gold_conll
  conll_test_path= ${data_dir}/test/test.chinese.v4_gold_conll
  checkpoint = ./trained_coref_model
  model_save_path = ./trained_coref_model
  pretrained_model = ./pretrain_model/chinese_roberta_l-6_h-768
  vocab_file = ./pretrain_model/chinese_roberta_l-6_h-768/vocab.txt
  bert_config_file = ./pretrain_model/chinese_roberta_l-6_h-768/config.json
}
chinese-lert-base {
  # Edit this
  data_dir = ./data

  # Computation limits.
  max_top_antecedents = 50
  max_training_sentences = 11
  top_span_ratio = 0.4
  max_num_speakers = 20
  max_segment_len = 128

  # Learning
  bert_learning_rate = 1e-05
  task_learning_rate = 0.0002
  adam_eps = 1e-6
  dropout_rate = 0.3

  # Task choice
  num_docs = 1810
  num_epochs = 1
  do_train = true
  do_eval = true
  do_test = true
  do_one_example_test = true
  eval_frequency = 5000
  report_frequency = 10

  # Model hyperparameters.
  genres = ["bc", "bn", "mz", "nw", "tc", "wb"]
  coref_depth = 2
  ffnn_size = 2000
  feature_size = 20
  max_span_width = 30
  use_metadata = true
  use_features = true
  use_segment_distance = true
  model_heads = false
  fine_grained = true
  use_prior = true
  single_example = true

  # file path
  train_path = ${data_dir}/train/train.chinese.128.jsonlines
  eval_path = ${data_dir}/dev/dev.chinese.128.jsonlines
  test_path = ${data_dir}/test/test.chinese.128.jsonlines
  test_output_path = ${data_dir}/test_result.jsonlines
  online_output_path = ${data_dir}/online_test_result.jsonlines
  conll_eval_path = ${data_dir}/dev/dev.chinese.v4_gold_conll
  conll_test_path= ${data_dir}/test/test.chinese.v4_gold_conll
  checkpoint = ./trained_coref_model/chinese-lert-base
  model_save_path = ./trained_coref_model/chinese-lert-base
  pretrained_model = ./pretrain_model/chinese-lert-base
  vocab_file = ./pretrain_model/chinese-lert-base/vocab.txt
  bert_config_file = ./pretrain_model/chinese-lert-base/config.json
}
chinese-lert-small {
  # Edit this
  data_dir = ./data

  # Computation limits.
  max_top_antecedents = 50
  max_training_sentences = 11
  top_span_ratio = 0.4
  max_num_speakers = 20
  max_segment_len = 128

  # Learning
  bert_learning_rate = 1e-05
  task_learning_rate = 0.0002
  adam_eps = 1e-6
  dropout_rate = 0.3

  # Task choice
  num_docs = 1810
  num_epochs = 30
  do_train = false
  do_eval = false
  do_test = false
  do_one_example_test = true
  eval_frequency = 5000
  report_frequency = 10

  # Model hyperparameters.
  genres = ["bc", "bn", "mz", "nw", "tc", "wb"]
  coref_depth = 2
  ffnn_size = 2000
  feature_size = 20
  max_span_width = 30
  use_metadata = true
  use_features = true
  use_segment_distance = true
  model_heads = false
  fine_grained = true
  use_prior = true
  single_example = true

  # file path
  train_path = ${data_dir}/train/train.chinese.128.jsonlines
  eval_path = ${data_dir}/dev/dev.chinese.128.jsonlines
  test_path = ${data_dir}/test/test.chinese.128.jsonlines
  test_output_path = ${data_dir}/test_result.jsonlines
  online_output_path = ${data_dir}/online_test_result.jsonlines
  conll_eval_path = ${data_dir}/dev/dev.chinese.v4_gold_conll
  conll_test_path= ${data_dir}/test/test.chinese.v4_gold_conll
  checkpoint = ./trained_coref_model/chinese-lert-small
  model_save_path = ./trained_coref_model/chinese-lert-small
  pretrained_model = ./pretrain_model/chinese-lert-small
  vocab_file = ./pretrain_model/chinese-lert-small/vocab.txt
  bert_config_file = ./pretrain_model/chinese-lert-small/config.json
}
ernie-3.0-medium-zh {
  # Edit this
  data_dir = ./data

  # Computation limits.
  max_top_antecedents = 50
  max_training_sentences = 11
  top_span_ratio = 0.4
  max_num_speakers = 20
  max_segment_len = 128

  # Learning
  bert_learning_rate = 1e-05
  task_learning_rate = 0.0002
  adam_eps = 1e-6
  dropout_rate = 0.3

  # Task choice
  num_docs = 1810
  num_epochs = 30
  do_train = true
  do_eval = true
  do_test = true
  do_one_example_test = true
  eval_frequency = 5000
  report_frequency = 10

  # Model hyperparameters.
  genres = ["bc", "bn", "mz", "nw", "tc", "wb"]
  coref_depth = 2
  ffnn_size = 2000
  feature_size = 20
  max_span_width = 30
  use_metadata = true
  use_features = true
  use_segment_distance = true
  model_heads = false
  fine_grained = true
  use_prior = true
  single_example = true

  # file path
  train_path = ${data_dir}/train/train.chinese.128.jsonlines
  eval_path = ${data_dir}/dev/dev.chinese.128.jsonlines
  test_path = ${data_dir}/test/test.chinese.128.jsonlines
  test_output_path = ${data_dir}/test_result.jsonlines
  online_output_path = ${data_dir}/online_test_result.jsonlines
  conll_eval_path = ${data_dir}/dev/dev.chinese.v4_gold_conll
  conll_test_path= ${data_dir}/test/test.chinese.v4_gold_conll
  checkpoint = ./trained_coref_model/ernie-3.0-medium-zh
  model_save_path = ./trained_coref_model/ernie-3.0-medium-zh
  pretrained_model = ./pretrain_model/ernie-3.0-medium-zh
  vocab_file = ./pretrain_model/ernie-3.0-medium-zh/vocab.txt
  bert_config_file = ./pretrain_model/ernie-3.0-medium-zh/config.json
}